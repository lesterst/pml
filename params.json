{"name":"Pml","tagline":"practical machine learning","body":"---\r\ntitle: \"pmlproject\"\r\nauthor: \"S Lester\"\r\ndate: \"August 21, 2015\"\r\noutput: html_document\r\n---\r\n\r\n\r\n  \r\n# Using Body Sensors to determine if Weight Lifting is Being Done Properly - A Reanalysis of data\r\n\r\nThis data is from a project that was done and reported online which used body sensors to try to determine if a person was using proper weight lifting technique.  The source is cited at the bottom of this paper.  The dataset from that project is being re-used for this course project.  \r\nFor this project, I downloaded the data from the coursera website, loaded it into R, evaluated and cleaned the data and then created a model using the random forest method.  Before creating the model I split the dataset into a training and a test set and used the test set to cross validate and determine out of sample error.  I then used the model to predict the exercise class on 20 unknown data samples.\r\n\r\n## Strategy (from lectures)\r\nPrediction study design:\r\n* Split data into training, testing, validation\r\n* on training set pick features using cross validation\r\n* on training set pick prediction function using cross validation\r\n* if no validation apply 1 x to the test set\r\n* if validation apply to test set and refine, apply 1x to validation\r\n\r\n## Data preparation \r\nThe data was downloaded from the coursera website.  It consists of 19,622 observations of 159 variables.  The final variable, \"classe\" is a factor labeled A,B,C,D or E.  This is the \"correct\" classification of the exercise as determined by the weight lifting instructor.  It represents either the properly done properly (A) or one of 4 common errors (B-E) and it is the answer we are trying to predict with the rest of the variables.  The rest of the variables consist of the output from the sensors worn on the subjects body, as well as some summary columns.  Many of the variables are blank in the dataset.\r\n\r\nThe first step was to clean the data.  This was done by removing the variables that are NA or blank or that represent times or identifiers.  I did this by using visual inspection of the table and then manually removing columns in a series of assignemnt statements  (using grepl).  The result was a dataset with 53 variables.\r\n\r\n```{r}\r\npml <- read.csv(\"pml-training.csv\")\r\n\r\nnafalse <- !is.na(pml[1,])\r\npml2 <- pml[,names(pml)[nafalse]]\r\n\r\nnokurt <- !grepl(\"kurtosis\", names(pml2))\r\npml2 <- pml2[,names(pml2)[nokurt]]\r\n\r\nnoskew <- !grepl(\"skewness\", names(pml2))\r\npml2 <- pml2[,names(pml2)[noskew]]\r\n\r\nnomin <- !grepl(\"min_yaw\", names(pml2))\r\npml2 <- pml2[,names(pml2)[nomin]]\r\n\r\nnomax <- !grepl(\"max_yaw\", names(pml2))\r\npml2 <- pml2[,names(pml2)[nomax]]\r\n\r\nnoamp <- !grepl(\"amplitude_yaw\", names(pml2))\r\npml2 <- pml2[,names(pml2)[noamp]]\r\n\r\nnostart <- names(pml2)[-(1:7)]\r\npml2 <- pml2[, nostart]\r\n\r\n```\r\n\r\n## Creating the model\r\nI then took a small subset of the data (1%) to test multiple different methods within the caret package to see what worked.  I settled on the random forest method because it ran, giving reasonable answers with reasonable accuracy in a reasonable period of time.  I then created the training data set using createDataPartition splitting the data into 60% training and 40% testing.  I then ran the analysis:\r\n\r\nThis took considerably longer.  \r\n```{r}\r\n\r\nlibrary(caret)\r\n\r\npmlsample <- createDataPartition(pml2$classe, p = 0.60, list=FALSE)\r\npmltrain <- pml2[pmlsample,]\r\npmltest <- pml2[-pmlsample,]\r\n\r\nfitpml <- train(classe ~ ., method = \"rf\", data=pmltrain)\r\n```\r\n\r\nThe results were as follows:\r\n```{r}\r\nRandom Forest \r\n\r\n11776 samples\r\n   52 predictor\r\n    5 classes: 'A', 'B', 'C', 'D', 'E' \r\n\r\nNo pre-processing\r\nResampling: Bootstrapped (25 reps) \r\nSummary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... \r\nResampling results across tuning parameters:\r\n\r\n  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   \r\n   2    0.9863724  0.9827557  0.002188335  0.002764943\r\n  27    0.9873154  0.9839493  0.002280896  0.002884525\r\n  52    0.9786747  0.9730150  0.003671623  0.004649611\r\n\r\nAccuracy was used to select the optimal model using  the largest value.\r\nThe final value used for the model was mtry = 27. \r\n\r\n```\r\n\r\nThe confusion matrix for the final model is as follows:\r\n\r\n```{r}\r\nCall:\r\n randomForest(x = x, y = y, mtry = param$mtry) \r\n               Type of random forest: classification\r\n                     Number of trees: 500\r\nNo. of variables tried at each split: 27\r\n\r\n        OOB estimate of  error rate: 0.84%\r\nConfusion matrix:\r\n     A    B    C    D    E  class.error\r\nA 3345    2    1    0    0 0.0008960573\r\nB   18 2251   10    0    0 0.0122860904\r\nC    0   16 2029    9    0 0.0121713729\r\nD    0    2   30 1897    1 0.0170984456\r\nE    0    2    5    3 2155 0.0046189376\r\n```\r\n\r\n## Estimating the out of sample error rate\r\n\r\nFrom the training data, the OOB estimate of error rate is 0.84%.  To measure the out of sample error rate using cross validation, I used the predict function to apply my model to the other 40% of the data.  I compared this predicted \"classe\"\" to the actual \"classe\"\" and converted to a percentage.\r\n\r\n```{r}\r\nsum(predict(fitpml, pmltest) == pmltest$classe)/length(pmltest$classe)\r\n[1] 0.9878919\r\n```\r\nThis gives an out of sample accuracy of 98.8%, or an out of sample error rate of 1.2% (which is a little worse than predicted from the training data).\r\n\r\n## Predicting the unknowns\r\n\r\nFinally we were asked to predict the classe for 20 unknowns.  This data was loaded and the same data cleaning strategy was applied.\r\n\r\n```{r}\r\npmltesting <- read.csv(\"pml-testing.csv\")\r\n\r\nnafalse <- !is.na(pmltesting[1,])\r\npml3 <- pmltesting[,names(pmltesting)[nafalse]]\r\n\r\nnokurt <- !grepl(\"kurtosis\", names(pml3))\r\npml3 <- pml3[,names(pml3)[nokurt]]\r\n\r\nnoskew <- !grepl(\"skewness\", names(pml3))\r\npml3 <- pml3[,names(pml3)[noskew]]\r\n\r\nnomin <- !grepl(\"min_yaw\", names(pml3))\r\npml3 <- pml3[,names(pml3)[nomin]]\r\n\r\nnomax <- !grepl(\"max_yaw\", names(pml3))\r\npml3 <- pml3[,names(pml3)[nomax]]\r\n\r\nnoamp <- !grepl(\"amplitude_yaw\", names(pml3))\r\npml3 <- pml3[,names(pml3)[noamp]]\r\n\r\nnostart <- names(pml3)[-(1:7)]\r\npml3 <- pml3[, nostart]\r\n```\r\n\r\n\r\nThe previous model was then used to predict the classe for each member of this cleaned testing dataset:\r\n\r\n```{r}\r\npredict(fitpml, pml3)\r\n\r\n[1] B A B A A E D B A A B C B A E E A B B B\r\n```\r\n\r\n## Conclusion\r\nThe data was cleaned and then used to create a model using random forest.  With this model, I was able to make predictions with an out of sample error rate of 1.2%.  I was able to use this model to make predictions for the unknown exercise types for the class.\r\n\r\n\r\nCitation of original source:\r\nUgulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. \r\n\r\nRead more: http://groupware.les.inf.puc-rio.br/har#ixzz3jV4FWDa5","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}