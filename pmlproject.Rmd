---
title: "pmlproject"
author: "S Lester"
date: "August 21, 2015"
output: html_document
---

Prediction study design:

  Split data into training, testing, validation
  
  on training set pick features using cross validation
  
  on training set pick prediction function using cross validation
  
  if no validation apply 1 x to the test set
  
  if validation applyt to test set and refine, apply 1x to validation
  
  
## Re-Analysis of Weight Lifting Data to Determine Quality of Exercise

The data is from a project that was done to try to determine if a person was using proper weight lifting technique using only body sensing measurements.  The data was collected and a model created by comparing the collected data to the "correct" answer as determined by an expert weightlifter.  Then the model was used to try to predict if the individual was lifting correctly or making an error of technique.  I am working through a replication of this analysis in this project.

The data was downloaded from the coursera website.  It consists of 19,622 observations of 159 variables.  The final variable, "classe" is a factor labeled A,B,C,D or E.  This is represents the the properly done exercise (A) or one of 4 common errors (B-E) and it is the answer we are trying to predict with the rest of the variables.  Many of the variables are blank.

The first step was to clean the data.  This was done by removing the variables that are NA or blank or that represent times or identifiers.  I did this by using visual inspection of the table and then manually removing columns in a series of assignemnt statements  (using grepl).  The result was a dataset with 53 variables.




```{r}
pml <- read.csv("pml-training.csv")

nafalse <- !is.na(pml[1,])
pml2 <- pml[,names(pml)[nafalse]]

nokurt <- !grepl("kurtosis", names(pml2))
pml2 <- pml2[,names(pml2)[nokurt]]

noskew <- !grepl("skewness", names(pml2))
pml2 <- pml2[,names(pml2)[noskew]]

nomin <- !grepl("min_yaw", names(pml2))
pml2 <- pml2[,names(pml2)[nomin]]

nomax <- !grepl("max_yaw", names(pml2))
pml2 <- pml2[,names(pml2)[nomax]]

noamp <- !grepl("amplitude_yaw", names(pml2))
pml2 <- pml2[,names(pml2)[noamp]]

nostart <- names(pml2)[-(1:7)]
pml2 <- pml2[, nostart]

```

I then took a small subset of the data (1%) to test multiple different methods within the caret package to see what worked.  I settled on the random forest method because it ran, giveing reasonable answers with reasonable accuracy in a reasonable period of time.  I then re-created the training data set using createDataPartition splitting the data into 60% training and 40% testing.  I then ran the analysis:

This took considerably longer.  
```{r}

library(caret)

pmlsample <- createDataPartition(pml2$classe, p = 0.60, list=FALSE)
pmltrain <- pml2[pmlsample,]
pmltest <- pml2[-pmlsample,]

fitpml <- train(classe ~ ., method = "rf", data=pmltrain)
```

The results were as follows:
```{r}
Random Forest 

11776 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9863724  0.9827557  0.002188335  0.002764943
  27    0.9873154  0.9839493  0.002280896  0.002884525
  52    0.9786747  0.9730150  0.003671623  0.004649611

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 

```


The confusion matrix for the final model is as follows:

```{r}
Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 27

        OOB estimate of  error rate: 0.84%
Confusion matrix:
     A    B    C    D    E  class.error
A 3345    2    1    0    0 0.0008960573
B   18 2251   10    0    0 0.0122860904
C    0   16 2029    9    0 0.0121713729
D    0    2   30 1897    1 0.0170984456
E    0    2    5    3 2155 0.0046189376
```



## Estimate the out of sample error rate

From the training data, the OOB estimate of error rate is 0.84%.  To measure the out of sample error rate using cross validation, I used the predict function to apply my model to the other 40% of the data.  I compared this predicted "classe"" to the actual "classe"" and converted to a percentage.

```{r}
sum(predict(fitpml, pmltest) == pmltest$classe)/length(pmltest$classe)
```
This gives an out of sample accuracy of 98.8%, or an out of sample error rate of 1.2% (which is a little worse than predicted from the training data).

## Predict the answers

Finally we were asked to predict the classe for 20 unknowns.  This data was loaded and the same data cleaning strategy was applied.

```{r}
pmltesting <- read.csv("pml-testing.csv")

nafalse <- !is.na(pmltesting[1,])
pml3 <- pmltesting[,names(pmltesting)[nafalse]]

nokurt <- !grepl("kurtosis", names(pml3))
pml3 <- pml3[,names(pml3)[nokurt]]

noskew <- !grepl("skewness", names(pml3))
pml3 <- pml3[,names(pml3)[noskew]]

nomin <- !grepl("min_yaw", names(pml3))
pml3 <- pml3[,names(pml3)[nomin]]

nomax <- !grepl("max_yaw", names(pml3))
pml3 <- pml3[,names(pml3)[nomax]]

noamp <- !grepl("amplitude_yaw", names(pml3))
pml3 <- pml3[,names(pml3)[noamp]]

nostart <- names(pml3)[-(1:7)]
pml3 <- pml3[, nostart]
```


The previous model was then used to predict the classe for each member of this cleaned testing dataset:

```{r}
predict(fitpml, pml3)

[1] B A B A A E D B A A B C B A E E A B B B
```

## Conclusion
The data was cleaned and then used to create a model using random forest.  With this model, I was able to make predictions with an out of sample error rate of 1.2%.  I was able to use this model to make predictions for the unknown exercise types for the class.